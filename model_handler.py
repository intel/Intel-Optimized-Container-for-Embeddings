# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: MIT

"""
A model handler for a custom text embedding model
that utilizes the Intel(r) Extension for Pytorch (IPEX)
framework for faster inference on Intel(r) Xeon(r) Platforms.
"""

import logging
import os
import torch
from ts.torch_handler.base_handler import BaseHandler
from ts.utils.util import PredictionException
from transformers import AutoTokenizer, AutoModel
import intel_extension_for_pytorch as ipex

logger = logging.getLogger(__name__)

class ModelHandler(BaseHandler):
    """
    https://pytorch.org/serve/custom_service.html#custom-handlers
    Based off the 'Custom handler with class level entry point' example.
    """
    def __init__(self):
        super().__init__()
        self.initialized = False
        self.tokenizer = None
        self._context = None

    def initialize(self, context):
        """
        Initialize model. This will be called during model loading time
        :param context: Initial context contains model server system properties.
        :return:
        """
        self._context = context
        properties = context.system_properties
        self.manifest = context.manifest

        model_dir = properties.get("model_dir")
        try:
            os.remove(os.path.join(model_dir,"pytorch_model.bin"))
        except OSError:
            pass
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModel.from_pretrained(model_dir, torchscript=True)
        self.model.eval()

        batch_size = 1
        seq_length=512
        vocab_size = self.model.config.vocab_size
        sample_input = {"input_ids": torch.randint(vocab_size, size=[batch_size, seq_length]),
                        "token_type_ids": torch.zeros(size=[batch_size, seq_length],
                                                      dtype=torch.int),
                        "attention_mask": torch.randint(1, size=[batch_size, seq_length])}

        self.model = ipex.optimize(self.model, level="O1",auto_kernel_selection=True,
                                   conv_bn_folding=False, dtype=torch.bfloat16)
        with torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False,
                                                     dtype=torch.bfloat16):
            self.model = torch.jit.trace(self.model, example_kwarg_inputs=sample_input,
                                         check_trace=False, strict=False)
            self.model = torch.jit.freeze(self.model)

        self.initialized = True

    def preprocess(self, data):
        """
        Transform raw input into model input data.
        :param batch: list of raw requests, should match batch size
        :return: list of preprocessed model input data
        """
        # extract input text from json
        try:
            text = data[0].get("data")
        except:
            raise PredictionException(
                "Invalid Input Format. Example Usage: \
                \n {\"instances\":[{\"data\": \"Example Input Text\"}]}",
                400)

        if not isinstance(text, str):
            raise PredictionException(
                "Invalid Input Format. Example Usage: \
                \n {\"instances\":[{\"data\": \"Example Input Text\"}]}",
                400)

        tokenized_text = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')
        return tokenized_text

    @staticmethod
    def mean_pooling(model_output, attention_mask):
        """
        Averages all token embeddings according to attention_mask
        :param model_output: token embeddings generated by the model
        :param attention_mask: attention weights generated by tokenizer (all weights are set to 1)
        """
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded,
                         1) / torch.clamp(input_mask_expanded.sum(1),
                                          min=1e-9)

    def inference(self, data, *args, **kwargs):
        """
        Internal inference methods
        :param data: tokenized text
        :return: pooled sentence embedding
        """
        # Call model and return pooled sentence embedding output
        with torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):
            model_output = self.model(**data)
            sentence_embeddings = self.mean_pooling(model_output, data['attention_mask'])
        return sentence_embeddings

    def postprocess(self, data):
        """
        Return inference result.
        :param data: list of inference output
        :return: list of predict results
        """
        # Take output from network and post-process to desired format
        return data[0].tolist()

    def handle(self, data, context):
        """
        Invoke by TorchServe for prediction request.
        Do pre-processing of data, prediction using model and postprocessing of prediciton output
        :param data: Input data for prediction
        :param context: Initial context contains model server system properties.
        :return: prediction output
        """

        try:
            model_input = self.preprocess(data)
            model_output = self.inference(model_input)
        except PredictionException as err:
            return err
        return [self.postprocess(model_output)]
