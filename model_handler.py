import torch
from ts.torch_handler.base_handler import BaseHandler
from transformers import AutoTokenizer, AutoModel
import intel_extension_for_pytorch as ipex 
import logging

logger = logging.getLogger(__name__)

class ModelHandler(BaseHandler):
    """
    A custom model handler implementation.
    """

    def __init__(self):
        super().__init__()
        self.initialized = False
        self.tokenizer = None

    def initialize(self, context):
        """
        Initialize model. This will be called during model loading time
        :param context: Initial context contains model server system properties.
        :return:
        """
        self._context = context
        # self.device = torch.device("cuda:" + str(properties.get("gpu_id")) if torch.cuda.is_available() else "cpu")
        properties = context.system_properties
        self.manifest = context.manifest

        model_dir = properties.get("model_dir")
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModel.from_pretrained(model_dir, torchscript=True)
        self.model.eval()

        batch_size = 1
        seq_length=512
        vocab_size = self.model.config.vocab_size
        sample_input = {"input_ids": torch.randint(vocab_size, size=[batch_size, seq_length]),
                        "token_type_ids": torch.zeros(size=[batch_size, seq_length], dtype=torch.int),
                        "attention_mask": torch.randint(1, size=[batch_size, seq_length])}

        self.model = ipex.optimize(self.model, level="O1",auto_kernel_selection=True, conv_bn_folding=False, dtype=torch.bfloat16)  
        with torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):
            self.model = torch.jit.trace(self.model, example_kwarg_inputs=sample_input, check_trace=False, strict=False)
            self.model = torch.jit.freeze(self.model)

        self.initialized = True

    def preprocess(self, data):
        """
        Transform raw input into model input data.
        :param batch: list of raw requests, should match batch size
        :return: list of preprocessed model input data
        """
        # Take the input data and make it inference ready
        text = data[0].get("data")
        if text is None:
            text = data[0].get("body")
            
        tokenized_text = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')
        return tokenized_text

    def mean_pooling(self, model_output, attention_mask):
        """
        Averages all token embeddings according to attention_mask
        :param model_output: token embeddings generated by the model
        :param attention_mask: attention weights generated by tokenizer (all weights are set to 1)
        """
        token_embeddings = model_output[0] #First element of model_output contains all token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def inference(self, tokenized_text):
        """
        Internal inference methods
        :param model_input: transformed model input data
        :return: list of inference output in NDArray
        """
        # Do some inference call to engine here and return output
        with torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):
            model_output = self.model(**tokenized_text)
            sentence_embeddings = self.mean_pooling(model_output, tokenized_text['attention_mask'])
        return sentence_embeddings

    def postprocess(self, model_output):
        """
        Return inference result.
        :param inference_output: list of inference output
        :return: list of predict results
        """
        # Take output from network and post-process to desired format
        return model_output[0].tolist()

    def handle(self, data, context):
        """
        Invoke by TorchServe for prediction request.
        Do pre-processing of data, prediction using model and postprocessing of prediciton output
        :param data: Input data for prediction
        :param context: Initial context contains model server system properties.
        :return: prediction output
        """
        
        model_input = self.preprocess(data)
        model_output = self.inference(model_input)
        # print(f"Model Output : {model_output[0].numpy().shape}")
        return [self.postprocess(model_output)]
